seed: 12345
log_path: "logs"
restore_from: null
hydra:
  run:
    dir: "." 
  output_subdir: null
mlflow:
  exp_name: "Training RAGN"
  exp_tags: null
  run_tags: null
  run_id: null
  get_last_run: false
data:
  train_data_size: 400000
  train_data_path: "data/train"
  val_data_path: "data/val"
  node_fields: null
train:
  compile: false
  num_epoch: 60
  init_lr: 1e-3
  end-lr: 1e-5
  power: 3 # The power of the polynomial decay related for learning rate.
  optimizer: "adam"
  decay_steps: 100000
  cycle: false # If the decay will cycling over and over
  train_batch_size: 128
  val_batch_size: 200
  msg_drop_ratio: 0.4 # Percentage of partial results from message passing that will not be consider in loss function
  class_weight: [0.4, 1.0] # The weight for each class (non-routing link and routing link)
  delta_time_to_validate: 30 # The interval time that the validation dataseet should be assessed.
  sacale_features: true
model:
  num_msg: 20
  enc_conf: [[[32, 8, 1, "SAME"], [8, 8, "VALID"]], [64, 32, 32]]
  mlp_conf: [64, 32, 32]
  lstm_conf: [32, 2]
  decision_conf: [2, 4]
  create_offset: true # Create trainable offset in the layer normalization.
  create_scale: true # Create trainable scale parameter in the layer normalization.
